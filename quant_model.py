# universal_model.py

import os
import zipfile
import gdown
import numpy as np
import onnxruntime as ort
from pathlib import Path
from functools import lru_cache
from typing import List, Union

from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer


class UniversalModel:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π:
    - ONNX –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (GDrive, HF, –ª–æ–∫–∞–ª—å–Ω–æ)
    - –û–±—ã—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ (transformers)
    - –ú–æ–¥–µ–ª–∏ SentenceTransformer

    –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –ø–∞–º—è—Ç–∏.
    """

    def __init__(self,
                 model_id: str,
                 model_type: str = "onnx",
                 source: str = "gdrive",
                 model_dir: str = "onnx_model",
                 model_file: str = "model_quantized.onnx",
                 tokenizer_name: str = None):
        """
        Args:
            model_id: ID –º–æ–¥–µ–ª–∏ (GDrive ID, HF repo_id –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å)
            model_type: "onnx", "transformers", "sentence-transformers"
            source: "gdrive", "hf", "local"
            model_dir: –ü–∞–ø–∫–∞ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è
            model_file: ONNX —Ñ–∞–π–ª
            tokenizer_name: –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ –æ—Ç–¥–µ–ª—å–Ω–æ)
        """
        self.model_id = model_id
        self.model_type = model_type
        self.source = source
        self.model_dir = model_dir
        self.model_file = model_file
        self.tokenizer_name = tokenizer_name or model_id

        self.session = None
        self.model = None
        self.tokenizer = None

        self._prepare_model()

    def _prepare_model(self):
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ç–æ—á–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏."""
        if self.model_type == "onnx":
            self._ensure_model_files()
        self._load_model_and_tokenizer()

    def _ensure_model_files(self):
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞ ONNX –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ."""
        model_path = Path(self.model_dir) / self.model_file
        if model_path.exists():
            return  # —É–∂–µ –µ—Å—Ç—å

        os.makedirs(self.model_dir, exist_ok=True)

        if self.source == "gdrive":
            zip_path = f"{self.model_dir}.zip"
            print(f"üì• –°–∫–∞—á–∏–≤–∞—é –º–æ–¥–µ–ª—å —Å Google Drive: {self.model_id}")
            gdown.download(f"https://drive.google.com/uc?id={self.model_id}", zip_path, quiet=False)
            with zipfile.ZipFile(zip_path, 'r') as zf:
                zf.extractall(self.model_dir)
            os.remove(zip_path)

        elif self.source == "hf":
            from huggingface_hub import snapshot_download
            print(f"üì• –°–∫–∞—á–∏–≤–∞—é –º–æ–¥–µ–ª—å —Å HF Hub: {self.model_id}")
            snapshot_download(repo_id=self.model_id, local_dir=self.model_dir, local_dir_use_symlinks=False)

        elif self.source == "local":
            print(f"üìÇ –ò—Å–ø–æ–ª—å–∑—É—é –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ {self.model_dir}")

        else:
            raise ValueError(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π source: {self.source}")

        print("‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞!")

    def _load_model_and_tokenizer(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞."""
        print(f"üöÄ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å —Ç–∏–ø–∞ {self.model_type}")

        if self.model_type == "onnx":
            model_path = str(Path(self.model_dir) / self.model_file)
            self.session = ort.InferenceSession(model_path, providers=["CPUExecutionProvider"])
            self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)

        elif self.model_type == "transformers":
            self.model = AutoModel.from_pretrained(self.model_id)
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)

        elif self.model_type == "sentence-transformers":
            self.model = SentenceTransformer(self.model_id)

        else:
            raise ValueError(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {self.model_type}")

        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

    def encode(self, texts: Union[str, List[str]], normalize_embeddings=True) -> np.ndarray:
        """–ö–æ–¥–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏."""
        if isinstance(texts, str):
            texts = [texts]

        if self.model_type == "onnx":
            inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors="np")
            ort_inputs = {k: v for k, v in inputs.items()}
            embeddings = self.session.run(None, ort_inputs)[0]
            embeddings = embeddings.mean(axis=1)

        elif self.model_type == "transformers":
            import torch
            self.model.eval()
            inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
            with torch.no_grad():
                outputs = self.model(**inputs)
                embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()

        elif self.model_type == "sentence-transformers":
            embeddings = self.model.encode(texts, convert_to_numpy=True)

        else:
            raise ValueError(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {self.model_type}")

        if normalize_embeddings:
            norms = np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-10
            embeddings = embeddings / norms
        return embeddings


# =========================
# üîπ –ì–ª–æ–±–∞–ª—å–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ –º–æ–¥–µ–ª–∏
# =========================
@lru_cache(maxsize=1)
def get_model():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å (—Å –∫—ç—à–µ–º –≤ –ø–∞–º—è—Ç–∏)."""
    model_id = os.getenv("MODEL_ID", "1lkrvCPIE1wvffIuCSHGtbEz3Epjx5R36")
    model_type = os.getenv("MODEL_TYPE", "onnx")
    source = os.getenv("MODEL_SOURCE", "gdrive")
    model_dir = os.getenv("MODEL_DIR", "onnx-user-bge-m3")
    tokenizer_name = os.getenv("TOKENIZER_NAME", "deepvk/USER-BGE-M3")

    return UniversalModel(
        model_id=model_id,
        model_type=model_type,
        source=source,
        model_dir=model_dir,
        tokenizer_name=tokenizer_name
    )
