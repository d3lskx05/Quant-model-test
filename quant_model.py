# universal_model.py
import os
import zipfile
import gdown
import numpy as np
import onnxruntime as ort
from pathlib import Path
from functools import lru_cache
from typing import List, Union

from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
import huggingface_hub
import torch


class UniversalModel:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –º–æ–¥–µ–ª–µ–π:
    - ONNX –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã–µ (GDrive, HF, –ª–æ–∫–∞–ª—å–Ω—ã–µ)
    - Transformers (HF)
    - Sentence-Transformers (HF)
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    """

    def __init__(self, model_id: str, model_type: str = "onnx", source: str = "gdrive",
                 model_dir: str = "onnx_model", tokenizer_name: str = None):
        """
        Args:
            model_id: ID –º–æ–¥–µ–ª–∏ (GDrive ID, HF repo_id –∏–ª–∏ –ø—É—Ç—å)
            model_type: "onnx", "transformers", "sentence-transformers"
            source: "gdrive", "hf", "local"
            model_dir: –ø–∞–ø–∫–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
            tokenizer_name: –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é = model_id)
        """
        self.model_id = model_id
        self.model_type = model_type
        self.source = source
        self.model_dir = Path(model_dir)
        self.tokenizer_name = tokenizer_name or model_id

        self.model_path = None
        self.model = None
        self.session = None
        self.tokenizer = None

        print(f"üîç –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è UniversalModel: type={model_type}, source={source}")
        self._prepare()

    # ========================
    # üîë –û—Å–Ω–æ–≤–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞
    # ========================
    def _prepare(self):
        if self.model_type == "onnx":
            print("üîπ –†–µ–∂–∏–º: ONNX –º–æ–¥–µ–ª—å")
            self._ensure_model_files()
            self.model_path = self._find_onnx_file()
            self.session = self._load_onnx_session()
            self.tokenizer = self._load_tokenizer()
        elif self.model_type == "transformers":
            print("üîπ –†–µ–∂–∏–º: Transformers –º–æ–¥–µ–ª—å")
            self.model = AutoModel.from_pretrained(self.model_id)
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
        elif self.model_type == "sentence-transformers":
            print("üîπ –†–µ–∂–∏–º: Sentence-Transformers –º–æ–¥–µ–ª—å")
            self.model = SentenceTransformer(self.model_id)
        else:
            raise ValueError(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {self.model_type}")

    # ========================
    # üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    # ========================
    def _ensure_model_files(self):
        os.makedirs(self.model_dir, exist_ok=True)
        if not any(self.model_dir.glob("*.onnx")):
            if self.source == "gdrive":
                zip_path = f"{self.model_dir}.zip"
                print(f"üì• –°–∫–∞—á–∏–≤–∞—é –º–æ–¥–µ–ª—å —Å Google Drive: {self.model_id}")
                gdown.download(f"https://drive.google.com/uc?id={self.model_id}", zip_path, quiet=False)
                with zipfile.ZipFile(zip_path, "r") as zf:
                    zf.extractall(self.model_dir)
                if os.path.exists(zip_path):  # ‚úÖ –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ
                    os.remove(zip_path)
            elif self.source == "hf":
                print(f"üì• –°–∫–∞—á–∏–≤–∞—é –º–æ–¥–µ–ª—å —Å Hugging Face: {self.model_id}")
                huggingface_hub.snapshot_download(
                    repo_id=self.model_id,
                    local_dir=self.model_dir,
                    local_dir_use_symlinks=False
                )
            elif self.source == "local":
                print(f"üìÇ –ò—Å–ø–æ–ª—å–∑—É—é –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å: {self.model_dir}")
            else:
                raise ValueError(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: {self.source}")
        else:
            print(f"‚úÖ –§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ —É–∂–µ –µ—Å—Ç—å –≤ {self.model_dir}")

    def _find_onnx_file(self):
        onnx_files = list(self.model_dir.rglob("*.onnx"))
        if not onnx_files:
            raise FileNotFoundError(f"‚ùå –í {self.model_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω .onnx —Ñ–∞–π–ª!")
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω ONNX —Ñ–∞–π–ª: {onnx_files[0]}")
        return onnx_files[0]

    def _load_onnx_session(self):
        so = ort.SessionOptions()
        so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        providers = ["CPUExecutionProvider"]
        try:
            if ort.get_device() == "GPU":
                providers.insert(0, "CUDAExecutionProvider")
        except Exception:
            pass
        print(f"üöÄ –ó–∞–≥—Ä—É–∂–∞—é ONNX –º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞—Ö: {providers}")
        return ort.InferenceSession(str(self.model_path), sess_options=so, providers=providers)

    def _load_tokenizer(self):
        try:
            return AutoTokenizer.from_pretrained(self.tokenizer_name)
        except Exception:
            try:
                return AutoTokenizer.from_pretrained(str(self.model_dir))
            except Exception:
                print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä, –∏—Å–ø–æ–ª—å–∑—É–µ–º deepvk/USER-BGE-M3")
                return AutoTokenizer.from_pretrained("deepvk/USER-BGE-M3")

    # ========================
    # üî• –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
    # ========================
    @lru_cache(maxsize=1024)
    def _encode_single(self, text: str, normalize: bool = True):
        if self.model_type == "onnx":
            inputs = self.tokenizer([text], padding=True, truncation=True, return_tensors="np")
            ort_inputs = {k: v for k, v in inputs.items()}
            emb = self.session.run(None, ort_inputs)[0]
        elif self.model_type == "transformers":
            self.model.eval()
            inputs = self.tokenizer([text], padding=True, truncation=True, return_tensors="pt")
            with torch.no_grad():
                outputs = self.model(**inputs)
                emb = outputs.last_hidden_state.mean(dim=1).cpu().numpy()
        elif self.model_type == "sentence-transformers":
            emb = self.model.encode([text], convert_to_numpy=True)
        else:
            raise ValueError("‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏")

        if normalize:
            norm = np.linalg.norm(emb, axis=1, keepdims=True) + 1e-10
            emb = emb / norm
        return emb[0]

    def encode(self, texts: Union[str, List[str]], normalize=True):
        if isinstance(texts, str):
            texts = [texts]
        return np.array([self._encode_single(t, normalize) for t in texts])


# ========================
# üîó –ì–ª–æ–±–∞–ª—å–Ω—ã–π –¥–æ—Å—Ç—É–ø
# ========================
@lru_cache(maxsize=1)
def get_model():
    model_id = os.getenv("MODEL_ID", "1lkrvCPIE1wvffIuCSHGtbEz3Epjx5R36")
    model_type = os.getenv("MODEL_TYPE", "onnx")  # onnx / transformers / sentence-transformers
    source = os.getenv("MODEL_SOURCE", "gdrive")
    model_dir = os.getenv("MODEL_DIR", "onnx-user-bge-m3")
    tokenizer = os.getenv("TOKENIZER_NAME", None)
    return UniversalModel(model_id, model_type, source, model_dir, tokenizer)
