import os
import zipfile
import gdown
import numpy as np
import onnxruntime as ort
from functools import lru_cache
from transformers import AutoTokenizer
import huggingface_hub
import psutil


class QuantizedSentenceModel:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ ONNX –º–æ–¥–µ–ª—è–º–∏
    –≤ —Å—Ç–∏–ª–µ SentenceTransformer. –ü–æ–¥–¥–µ—Ä–∂–∫–∞:
      - Google Drive (–ø–æ ID)
      - Hugging Face Hub
      - –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
    """

    def __init__(self, model_id: str, source: str = "gdrive",
                 model_dir: str = "onnx_model", model_file: str = "model_quantized.onnx"):
        self.model_id = model_id
        self.source = source  # "gdrive", "hf", "local"
        self.model_dir = model_dir
        self.model_file = model_file
        self.model_path = os.path.join(self.model_dir, self.model_file)

        self._ensure_model()
        self.session = self._load_session()
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_dir)

    def _ensure_model(self):
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç source."""
        if os.path.exists(self.model_path):
            return  # –£–∂–µ –µ—Å—Ç—å

        os.makedirs(self.model_dir, exist_ok=True)

        if self.source == "gdrive":
            zip_path = f"{self.model_dir}.zip"
            print(f"üì• –°–∫–∞—á–∏–≤–∞—é –º–æ–¥–µ–ª—å —Å Google Drive: {self.model_id}")
            url = f"https://drive.google.com/uc?id={self.model_id}"
            gdown.download(url, zip_path, quiet=False)
            print(f"üì¶ –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ {zip_path}...")
            with zipfile.ZipFile(zip_path, 'r') as zf:
                zf.extractall(self.model_dir)
            os.remove(zip_path)

        elif self.source == "hf":
            print(f"üì• –°–∫–∞—á–∏–≤–∞—é –º–æ–¥–µ–ª—å —Å Hugging Face: {self.model_id}")
            huggingface_hub.snapshot_download(
                repo_id=self.model_id,
                local_dir=self.model_dir,
                local_dir_use_symlinks=False
            )

        elif self.source == "local":
            print(f"üìÇ –ò—Å–ø–æ–ª—å–∑—É—é –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å: {self.model_dir}")
            # –ù–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–µ–º
        else:
            raise ValueError(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π source: {self.source}")

        print("‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞!")

    def _load_session(self):
        """–°–æ–∑–¥–∞—ë–º ONNX Runtime Session —Å –∞–≤—Ç–æ-–≤—ã–±–æ—Ä–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞."""
        so = ort.SessionOptions()
        so.intra_op_num_threads = 1
        so.inter_op_num_threads = 1
        so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

        # –ï—Å–ª–∏ –µ—Å—Ç—å GPU, –∏—Å–ø–æ–ª—å–∑—É–µ–º CUDA
        providers = ["CPUExecutionProvider"]
        try:
            if ort.get_device() == "GPU":
                providers.insert(0, "CUDAExecutionProvider")
        except Exception:
            pass

        print(f"üöÄ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å {self.model_file} —Å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏: {providers}")
        return ort.InferenceSession(self.model_path, sess_options=so, providers=providers)

    @lru_cache(maxsize=1024)
    def _encode_cached(self, texts_tuple, normalize_embeddings):
        """–ö—ç—à–∏—Ä—É–µ–º—ã–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ (–≤ –ø–∞–º—è—Ç–∏)."""
        texts = list(texts_tuple)
        inputs = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            return_tensors="np"
        )
        ort_inputs = {k: v for k, v in inputs.items()}
        embeddings = self.session.run(None, ort_inputs)[0]

        if normalize_embeddings:
            norms = np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-10
            embeddings = embeddings / norms
        return embeddings

    def encode(self, texts, normalize_embeddings=True):
        """–ü—É–±–ª–∏—á–Ω—ã–π –º–µ—Ç–æ–¥ encode —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤ –ø–∞–º—è—Ç–∏."""
        if isinstance(texts, str):
            texts = [texts]
        texts_tuple = tuple(texts)
        return self._encode_cached(texts_tuple, normalize_embeddings)

    # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –¥–∏—Å–∫–æ–≤–æ–≥–æ –∫—ç—à–∞
    # def _encode_disk_cache(self, texts, normalize_embeddings=True):
    #     """
    #     TODO: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–∏—Å–∫–æ–≤–æ–≥–æ –∫—ç—à–∞.
    #     –ú–æ–∂–Ω–æ —Ö—ç—à–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ npy-—Ñ–∞–π–ª—ã.
    #     """
    #     pass

    @staticmethod
    def profile():
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤."""
        process = psutil.Process(os.getpid())
        mem_info = process.memory_info()
        cpu = psutil.cpu_percent(interval=0.1)
        ram = psutil.virtual_memory()
        return {
            "cpu_percent": cpu,
            "ram_percent": ram.percent,
            "ram_mb": mem_info.rss / 1024**2
        }


@lru_cache(maxsize=1)
def get_model():
    """–ì–ª–æ–±–∞–ª—å–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ –º–æ–¥–µ–ª–∏ —Å –∫—ç—à–µ–º."""
    model_id = os.getenv("MODEL_ID", "1lkrvCPIE1wvffIuCSHGtbEz3Epjx5R36")
    source = os.getenv("MODEL_SOURCE", "gdrive")  # "gdrive", "hf", "local"
    return QuantizedSentenceModel(model_id, source)
