# app.py
import streamlit as st
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModel
from quant_model import QuantModel


# =============================
# ‚ö° –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
# =============================

@st.cache_resource
def load_models():
    # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è FP32 –º–æ–¥–µ–ª—å
    orig_id = "deepvk/USER-bge-m3"
    orig_tokenizer = AutoTokenizer.from_pretrained(orig_id)
    orig_model = AutoModel.from_pretrained(orig_id)

    # –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è INT8 –º–æ–¥–µ–ª—å
    quant = QuantModel("skatzR/USER-BGE-M3-ONNX-INT8")

    return orig_tokenizer, orig_model, quant


orig_tokenizer, orig_model, quant_model = load_models()


# =============================
# ‚ö° –£—Ç–∏–ª–∏—Ç—ã
# =============================

def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0]
    mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return (token_embeddings * mask).sum(1) / torch.clamp(mask.sum(1), min=1e-9)


def encode_orig(texts):
    inputs = orig_tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = orig_model(**inputs)
    emb = mean_pooling(outputs, inputs["attention_mask"])
    emb = torch.nn.functional.normalize(emb, p=2, dim=1)
    return emb.cpu().numpy()


# =============================
# ‚ö° Streamlit UI
# =============================

st.title("üß© DeepVK-BGE-M3 ‚Äî FP32 vs Quantized ONNX")
st.write("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –∏ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")

text1 = st.text_input("–¢–µ–∫—Å—Ç 1", "–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä!")
text2 = st.text_input("–¢–µ–∫—Å—Ç 2", "Hello, world!")

if st.button("üîé –ü—Ä–æ–≤–µ—Ä–∏—Ç—å"):
    texts = [text1, text2]

    # FP32
    emb_orig = encode_orig(texts)

    # INT8 ONNX
    emb_quant = quant_model.encode(texts)

    # Cosine similarity –≤–Ω—É—Ç—Ä–∏ –º–æ–¥–µ–ª–µ–π
    sim_orig = float(np.dot(emb_orig[0], emb_orig[1]))
    sim_quant = float(np.dot(emb_quant[0], emb_quant[1]))

    # Cosine similarity –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏
    cross_sim = float(np.dot(emb_orig[0], emb_quant[0]))

    st.subheader("üìê Cosine Similarities")
    st.write(f"**FP32 model:** {sim_orig:.4f}")
    st.write(f"**Quantized INT8 model:** {sim_quant:.4f}")
    st.write(f"**FP32 vs INT8 (cross-check):** {cross_sim:.4f}")
